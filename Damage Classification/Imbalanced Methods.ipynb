{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Imbalanced learning package\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the bird strike data, and drop a number of columns that will be unused in this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (14,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"wildlife.csv\",parse_dates = ['INCIDENT_DATE '])\n",
    "df_copy=df.copy().drop(columns=[\"TYPE_ENG\",\"AMA\",\"AMO\",\"EMA\",\"EMO\",\"AC_CLASS\",\"AC_MASS\",\"REG\",\"REMAINS_COLLECTED\",\"REMAINS_SENT\",\"RUNWAY\",\"STR_RAD\",\"DAM_RAD\",\"STR_WINDSHLD\",\"STR_NOSE\",\"DAM_NOSE\",\"STR_ENG1\",\"DAM_ENG1\",\"STR_ENG2\",\"DAM_ENG2\",\"STR_ENG3\",\"DAM_ENG3\",\"STR_ENG4\",\"DAM_ENG4\",\"STR_PROP\",\"DAM_PROP\",\"STR_WING_ROT\",\"DAM_WING_ROT\",\"STR_FUSE\",\"DAM_FUSE\",\"STR_LG\",\"DAM_LG\",\"STR_TAIL\",\"DAM_TAIL\",\"STR_LGHTS\",\"DAM_LGHTS\",\"STR_OTHER\",\"DAM_OTHER\",\"OTHER_SPECIFY\",\"REPORTED_NAME\",\"REPORTED_TITLE\",\"TRANSFER\", \"Unnamed: 101\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compile an (incomplete) list of species that are small, medium, and large, as we will use that as a factor in predicting damage or the severity of damage. These are one-hot encoded and stored. Unlisted species are treated as unknown size, and form the class that is dropped in the encoding. Addition of further species to these lists could potentially be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "smalls = ['Barn swallow','Horned lark','Cliff swallow','Sparrows','Unknown bird - small']\n",
    "df_copy['small'] = df_copy.SPECIES.isin(smalls).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "meds = ['Unknown bird - medium','Mourning dove','Killdeer',\n",
    "        'American kestrel','European starling','Rock pigeon',\n",
    "        'Eastern meadowlark','Western meadowlark','Ring-billed gull','American robin','Barn owl']\n",
    "df_copy['medium'] = df_copy.SPECIES.isin(meds).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "larges = ['Unknown bird - large','Gulls','Red-tailed hawk']\n",
    "df_copy['large'] = df_copy.SPECIES.isin(larges).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discovered in other analyses, a large portion of strikes occur on approach to landing, and thus we include this as as input with the hope that it can help predict the outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['on_approach'] = df_copy.PHASE_OF_FLIGHT.isin(['Approach']).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also consider the height at which the strike occurred. There are a number of NA values, which we replace with 0 to allow for numerical data analysis here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.HEIGHT = df_copy.HEIGHT.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to consider the weather, which is already one-hot encoded in the various PRECIP columns. In addition, other analyses showed that if bird was sucked into an engine, more monetary damage was likely, meaning we also include INGESTED in our input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we one hot encode the two variables we hope to predict: Whether the strike resulted in damage, as well as if the damage was substantial or worse. The 'Class' designations are military designations, and were matched based on estimated value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['damaged'] = df_copy.DAMAGE.isin(['S','D','M','M?','Class E','Class C','Class B']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of strikes that resulted in damage is: 0.06256164576581655\n"
     ]
    }
   ],
   "source": [
    "print('The proportion of strikes that resulted in damage is:',sum(df_copy.damaged)/len(df_copy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the proportion of stikes that result in damage is very small, meaning that always predicting that there is no strike gives 93% accuracy. However, this is not helpful--we want to be able to predict a nonzero amount of damaging strikes without losing accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is sufficient data to have a generous amount of training data in the train-test split, though we are careful to stratify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_copy[['small','medium','large','on_approach','HEIGHT','PRECIP_SNOW','PRECIP_RAIN','PRECIP_FOG','INGESTED']],\n",
    "    df_copy['damaged'], test_size=0.1, random_state=440, stratify = df_copy['damaged'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imbalance in size makes basic classifiers insufficient in predicting damage from strikes, as shown by this fairly large fully connected neural net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(20, 20, 20, 20, 20), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=4000,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20,20,20),max_iter=4000)\n",
    "mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9391138632302368\n",
      "Test accuracy: 0.9385871149548148\n",
      "True positives in training: 0.07706247842595788\n",
      "True positives in testing: 0.06599378881987578\n",
      "True negatives in training: 0.9966425366988592\n",
      "True negatives in testing: 0.9968383953560692\n"
     ]
    }
   ],
   "source": [
    "pred_train = mlp.predict(X_train)\n",
    "pred_test = mlp.predict(X_test)\n",
    "\n",
    "print('Train accuracy:', sum(pred_train ==y_train)/len(y_train))\n",
    "print('Test accuracy:', sum(pred_test ==y_test)/len(y_test))\n",
    "print('True positives in training:', sum((pred_train==1) & (y_train))/sum(y_train))\n",
    "print('True positives in testing:', sum((pred_test==1) & (y_test))/sum(y_test))\n",
    "print('True negatives in training:', sum((pred_train==0) & (1-y_train))/sum(1-y_train))\n",
    "print('True negatives in testing:', sum((pred_test==0) & (1-y_test))/sum(1-y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fairly large network is essentially a constant classfier, predicting very few instances of damage, without great accuracy. The network is unable to find any way to meaningfully distingush the classes, since it mostly only learns about the strikes with no damage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address the small proportion of strikes, we can use a balaced version of bagging to undersample the non-damaging strikes and raise the importance of the damaging strikes in a manner analogous to ADABoost. This will increase the numebr of true positives at the cost of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                                class_weight=None,\n",
       "                                                                criterion='gini',\n",
       "                                                                max_depth=20,\n",
       "                                                                max_features=None,\n",
       "                                                                max_leaf_nodes=None,\n",
       "                                                                min_impurity_decrease=0.0,\n",
       "                                                                min_impurity_split=None,\n",
       "                                                                min_samples_leaf=1,\n",
       "                                                                min_samples_split=2,\n",
       "                                                                min_weight_fraction_leaf=0.0,\n",
       "                                                                presort='deprecated',\n",
       "                                                                random_state=None,\n",
       "                                                                splitter='best'),\n",
       "                          bootstrap=True, bootstrap_features=False,\n",
       "                          max_features=1.0, max_samples=2000, n_estimators=50,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          replacement=False, sampling_strategy='auto',\n",
       "                          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bal_bag = BalancedBaggingClassifier(DecisionTreeClassifier(max_depth = 20) , n_estimators = 50,\n",
    "                                   bootstrap = True, max_samples = 2000)\n",
    "\n",
    "bal_bag.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7904778357834272\n",
      "Test accuracy: 0.7916140316781654\n",
      "True positives in training: 0.6517086641353124\n",
      "True positives in testing: 0.6583850931677019\n",
      "True negatives in training: 0.7997385440242336\n",
      "True negatives in testing: 0.800507929926402\n"
     ]
    }
   ],
   "source": [
    "pred_train = bal_bag.predict(X_train)\n",
    "pred_test = bal_bag.predict(X_test)\n",
    "\n",
    "print('Train accuracy:', sum(pred_train ==y_train)/len(y_train))\n",
    "print('Test accuracy:', sum(pred_test ==y_test)/len(y_test))\n",
    "print('True positives in training:', sum((pred_train==1) & (y_train))/sum(y_train))\n",
    "print('True positives in testing:', sum((pred_test==1) & (y_test))/sum(y_test))\n",
    "print('True negatives in training:', sum((pred_train==0) & (1-y_train))/sum(1-y_train))\n",
    "print('True negatives in testing:', sum((pred_test==0) & (1-y_test))/sum(1-y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EasyEnsembleClassifier is a built-in method for doing this kind of undersampling, and perfoms similarly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EasyEnsembleClassifier(base_estimator=None, n_estimators=50, n_jobs=None,\n",
       "                       random_state=None, replacement=False,\n",
       "                       sampling_strategy='auto', verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EasyEnsembleClassifier(n_estimators=50)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7951908697788167\n",
      "Test accuracy: 0.796861335147216\n",
      "True positives in training: 0.6474801518812565\n",
      "True positives in testing: 0.6506211180124224\n",
      "True negatives in training: 0.805048288730326\n",
      "True negatives in testing: 0.8066238208769566\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.predict(X_train)\n",
    "pred_test = model.predict(X_test)\n",
    "\n",
    "print('Train accuracy:', sum(pred_train ==y_train)/len(y_train))\n",
    "print('Test accuracy:', sum(pred_test ==y_test)/len(y_test))\n",
    "print('True positives in training:', sum((pred_train==1) & (y_train))/sum(y_train))\n",
    "print('True positives in testing:', sum((pred_test==1) & (y_test))/sum(y_test))\n",
    "print('True negatives in training:', sum((pred_train==0) & (1-y_train))/sum(1-y_train))\n",
    "print('True negatives in testing:', sum((pred_test==0) & (1-y_test))/sum(1-y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of undersampling, we can also try balancing the classes by oversampling using the ADASYN method, based on ADABoost. We will use the same size neural net as before, but now we have a much better rate of true positives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_pipe = Pipeline([('adasyn', ADASYN(random_state=440)),('mlp', MLPClassifier(hidden_layer_sizes=(10,10,10),max_iter=1000))])\n",
    "#mlp_pipe = Pipeline([('adasyn', ADASYN()),('mlp', MLPClassifier(hidden_layer_sizes=(10,10,10),max_iter=1000))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('adasyn',\n",
       "                 ADASYN(n_jobs=None, n_neighbors=5, random_state=440,\n",
       "                        sampling_strategy='auto')),\n",
       "                ('mlp',\n",
       "                 MLPClassifier(activation='relu', alpha=0.0001,\n",
       "                               batch_size='auto', beta_1=0.9, beta_2=0.999,\n",
       "                               early_stopping=False, epsilon=1e-08,\n",
       "                               hidden_layer_sizes=(10, 10, 10),\n",
       "                               learning_rate='constant',\n",
       "                               learning_rate_init=0.001, max_fun=15000,\n",
       "                               max_iter=1000, momentum=0.9, n_iter_no_change=10,\n",
       "                               nesterovs_momentum=True, power_t=0.5,\n",
       "                               random_state=None, shuffle=True, solver='adam',\n",
       "                               tol=0.0001, validation_fraction=0.1,\n",
       "                               verbose=False, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7587822772646048\n",
      "Test accuracy: 0.7610533475852687\n",
      "True positives in training: 0.6811356575768036\n",
      "True positives in testing: 0.6824534161490683\n",
      "True negatives in training: 0.7639639950933812\n",
      "True negatives in testing: 0.7663004042707577\n"
     ]
    }
   ],
   "source": [
    "pred_train = mlp_pipe.predict(X_train)\n",
    "pred_test = mlp_pipe.predict(X_test)\n",
    "\n",
    "print('Train accuracy:', sum(pred_train ==y_train)/len(y_train))\n",
    "print('Test accuracy:', sum(pred_test ==y_test)/len(y_test))\n",
    "print('True positives in training:', sum((pred_train==1) & (y_train))/sum(y_train))\n",
    "print('True positives in testing:', sum((pred_test==1) & (y_test))/sum(y_test))\n",
    "print('True negatives in training:', sum((pred_train==0) & (1-y_train))/sum(1-y_train))\n",
    "print('True negatives in testing:', sum((pred_test==0) & (1-y_test))/sum(1-y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternate way to get around this tradeoff is to change gears a bit, and only examine the strikes that caused damage in an attempt to predict when the damage is severe. Notice that this is a much larger proportion, though still small enough to need to stratify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['high_damage'] = df_copy.DAMAGE.isin(['S','D','Class C','Class B']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of damaging strikes that resulted in high damage is: 0.21186703945324634\n"
     ]
    }
   ],
   "source": [
    "print('The proportion of damaging strikes that resulted in high damage is:', sum(df_copy.high_damage)/sum(df_copy.damaged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we restrict our dataset to only these entries, and create a stratified train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_damage = df_copy.copy()[df_copy.damaged > 0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_damage[['small','medium','large','on_approach','HEIGHT','PRECIP_SNOW','PRECIP_RAIN','PRECIP_FOG','INGESTED']],\n",
    "    df_damage['high_damage'], test_size=0.1, random_state=440, stratify = df_damage['high_damage'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use various classifiers here to attempt to classify the severity of the damage. Note that the MLP classifier is still mostly unable to exceed a fixed guess, but tree and forest classifiers can do better, likely due to the categorical nature of the input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(50, 50, 50, 50, 50, 50, 50, 50),\n",
       "              learning_rate='constant', learning_rate_init=0.001, max_fun=15000,\n",
       "              max_iter=10000, momentum=0.9, n_iter_no_change=10,\n",
       "              nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "              shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(50,50,50,50,50,50,50,50),max_iter=10000)\n",
    "mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7880566102865033\n",
      "Test accuracy: 0.781832298136646\n",
      "True positives in training: 0.2920570264765784\n",
      "True positives in testing: 0.31868131868131866\n",
      "True negatives in training: 0.9213839921165006\n",
      "True negatives in testing: 0.9064039408866995\n"
     ]
    }
   ],
   "source": [
    "pred_train = mlp.predict(X_train)\n",
    "pred_test = mlp.predict(X_test)\n",
    "\n",
    "print('Train accuracy:', sum(pred_train ==y_train)/len(y_train))\n",
    "print('Test accuracy:', sum(pred_test ==y_test)/len(y_test))\n",
    "print('True positives in training:', sum((pred_train==1) & (y_train))/sum(y_train))\n",
    "print('True positives in testing:', sum((pred_test==1) & (y_test))/sum(y_test))\n",
    "print('True negatives in training:', sum((pred_train==0) & (1-y_train))/sum(1-y_train))\n",
    "print('True negatives in testing:', sum((pred_test==0) & (1-y_test))/sum(1-y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=20, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=20)\n",
    "tree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.8187780462547463\n",
      "Test accuracy: 0.8051242236024845\n",
      "True positives in training: 0.23136456211812628\n",
      "True positives in testing: 0.20146520146520147\n",
      "True negatives in training: 0.9766779809482098\n",
      "True negatives in testing: 0.967487684729064\n"
     ]
    }
   ],
   "source": [
    "pred_train = tree.predict(X_train)\n",
    "pred_test = tree.predict(X_test)\n",
    "\n",
    "print('Train accuracy:', sum(pred_train ==y_train)/len(y_train))\n",
    "print('Test accuracy:', sum(pred_test ==y_test)/len(y_test))\n",
    "print('True positives in training:', sum((pred_train==1) & (y_train))/sum(y_train))\n",
    "print('True positives in testing:', sum((pred_test==1) & (y_test))/sum(y_test))\n",
    "print('True negatives in training:', sum((pred_train==0) & (1-y_train))/sum(1-y_train))\n",
    "print('True negatives in testing:', sum((pred_test==0) & (1-y_test))/sum(1-y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=10, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(max_depth = 10, n_estimators = 100)\n",
    "forest.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.8107525025888851\n",
      "Test accuracy: 0.8012422360248447\n",
      "True positives in training: 0.21221995926680245\n",
      "True positives in testing: 0.19047619047619047\n",
      "True negatives in training: 0.9716413007774006\n",
      "True negatives in testing: 0.9655172413793104\n"
     ]
    }
   ],
   "source": [
    "pred_train = forest.predict(X_train)\n",
    "pred_test = forest.predict(X_test)\n",
    "\n",
    "print('Train accuracy:', sum(pred_train ==y_train)/len(y_train))\n",
    "print('Test accuracy:', sum(pred_test ==y_test)/len(y_test))\n",
    "print('True positives in training:', sum((pred_train==1) & (y_train))/sum(y_train))\n",
    "print('True positives in testing:', sum((pred_test==1) & (y_test))/sum(y_test))\n",
    "print('True negatives in training:', sum((pred_train==0) & (1-y_train))/sum(1-y_train))\n",
    "print('True negatives in testing:', sum((pred_test==0) & (1-y_test))/sum(1-y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can attempt to boost this with similar techniques, including a balanced bagging approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                                class_weight=None,\n",
       "                                                                criterion='gini',\n",
       "                                                                max_depth=10,\n",
       "                                                                max_features=None,\n",
       "                                                                max_leaf_nodes=None,\n",
       "                                                                min_impurity_decrease=0.0,\n",
       "                                                                min_impurity_split=None,\n",
       "                                                                min_samples_leaf=1,\n",
       "                                                                min_samples_split=2,\n",
       "                                                                min_weight_fraction_leaf=0.0,\n",
       "                                                                presort='deprecated',\n",
       "                                                                random_state=None,\n",
       "                                                                splitter='best'),\n",
       "                          bootstrap=True, bootstrap_features=False,\n",
       "                          max_features=1.0, max_samples=2000, n_estimators=50,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          replacement=False, sampling_strategy='auto',\n",
       "                          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bal_bag = BalancedBaggingClassifier(DecisionTreeClassifier(max_depth = 10) , n_estimators = 50,\n",
    "                                   bootstrap = True, max_samples = 2000)\n",
    "\n",
    "bal_bag.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7406800138073869\n",
      "Test accuracy: 0.7336956521739131\n",
      "True positives in training: 0.624847250509165\n",
      "True positives in testing: 0.63003663003663\n",
      "True negatives in training: 0.7718164896529071\n",
      "True negatives in testing: 0.761576354679803\n"
     ]
    }
   ],
   "source": [
    "pred_train = bal_bag.predict(X_train)\n",
    "pred_test = bal_bag.predict(X_test)\n",
    "\n",
    "print('Train accuracy:', sum(pred_train ==y_train)/len(y_train))\n",
    "print('Test accuracy:', sum(pred_test ==y_test)/len(y_test))\n",
    "print('True positives in training:', sum((pred_train==1) & (y_train))/sum(y_train))\n",
    "print('True positives in testing:', sum((pred_test==1) & (y_test))/sum(y_test))\n",
    "print('True negatives in training:', sum((pred_train==0) & (1-y_train))/sum(1-y_train))\n",
    "print('True negatives in testing:', sum((pred_test==0) & (1-y_test))/sum(1-y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this instance, we can do substantially better in true positives, although our overall accuracy is actually worse than a constant predictor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
